{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5c1122f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5c1122f0",
        "outputId": "9f284a51-c9ef-40da-aa97-3463357f2f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package java-common.\n",
            "(Reading database ... 125080 files and directories currently installed.)\n",
            "Preparing to unpack .../java-common_0.72build2_all.deb ...\n",
            "Unpacking java-common (0.72build2) ...\n",
            "Selecting previously unselected package libpcsclite1:amd64.\n",
            "Preparing to unpack .../libpcsclite1_1.9.5-3ubuntu1_amd64.deb ...\n",
            "Unpacking libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package ca-certificates-java.\n",
            "Preparing to unpack .../ca-certificates-java_20190909ubuntu1.2_all.deb ...\n",
            "Unpacking ca-certificates-java (20190909ubuntu1.2) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up java-common (0.72build2) ...\n",
            "Setting up libpcsclite1:amd64 (1.9.5-3ubuntu1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jfr to provide /usr/bin/jfr (jfr) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "Setting up ca-certificates-java (20190909ubuntu1.2) ...\n",
            "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
            "Adding debian:TWCA_Root_Certification_Authority.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
            "Adding debian:Security_Communication_RootCA2.pem\n",
            "Adding debian:Izenpe.com.pem\n",
            "Adding debian:Certainly_Root_E1.pem\n",
            "Adding debian:GlobalSign_Root_R46.pem\n",
            "Adding debian:Secure_Global_CA.pem\n",
            "Adding debian:ANF_Secure_Server_Root_CA.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
            "Adding debian:AffirmTrust_Premium_ECC.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
            "Adding debian:GlobalSign_Root_CA_-_R6.pem\n",
            "Adding debian:Amazon_Root_CA_3.pem\n",
            "Adding debian:GTS_Root_R1.pem\n",
            "Adding debian:GTS_Root_R2.pem\n",
            "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
            "Adding debian:Trustwave_Global_ECC_P256_Certification_Authority.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
            "Adding debian:certSIGN_ROOT_CA.pem\n",
            "Adding debian:Certum_Trusted_Network_CA.pem\n",
            "Adding debian:HiPKI_Root_CA_-_G1.pem\n",
            "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
            "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
            "Adding debian:emSign_Root_CA_-_G1.pem\n",
            "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
            "Adding debian:Starfield_Class_2_CA.pem\n",
            "Adding debian:GTS_Root_R4.pem\n",
            "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
            "Adding debian:Amazon_Root_CA_2.pem\n",
            "Adding debian:emSign_Root_CA_-_C1.pem\n",
            "Adding debian:Amazon_Root_CA_1.pem\n",
            "Adding debian:Atos_TrustedRoot_2011.pem\n",
            "Adding debian:ACCVRAIZ1.pem\n",
            "Adding debian:Actalis_Authentication_Root_CA.pem\n",
            "Adding debian:Comodo_AAA_Services_root.pem\n",
            "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
            "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
            "Adding debian:TunTrust_Root_CA.pem\n",
            "Adding debian:HARICA_TLS_RSA_Root_CA_2021.pem\n",
            "Adding debian:ISRG_Root_X2.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM_SERVIDORES_SEGUROS.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_G3.pem\n",
            "Adding debian:Buypass_Class_2_Root_CA.pem\n",
            "Adding debian:Certum_EC-384_CA.pem\n",
            "Adding debian:SZAFIR_ROOT_CA2.pem\n",
            "Adding debian:Certainly_Root_R1.pem\n",
            "Adding debian:AffirmTrust_Premium.pem\n",
            "Adding debian:Certum_Trusted_Root_CA.pem\n",
            "Adding debian:emSign_ECC_Root_CA_-_C3.pem\n",
            "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
            "Adding debian:AffirmTrust_Networking.pem\n",
            "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
            "Adding debian:Microsoft_ECC_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:Security_Communication_Root_CA.pem\n",
            "Adding debian:vTrus_Root_CA.pem\n",
            "Adding debian:Baltimore_CyberTrust_Root.pem\n",
            "Adding debian:Certigna_Root_CA.pem\n",
            "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
            "Adding debian:NetLock_Arany_=Class_Gold=_F≈ëtan√∫s√≠tv√°ny.pem\n",
            "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority_-_G4.pem\n",
            "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
            "Adding debian:Go_Daddy_Class_2_CA.pem\n",
            "Adding debian:DigiCert_Global_Root_CA.pem\n",
            "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
            "Adding debian:DigiCert_TLS_ECC_P384_Root_G5.pem\n",
            "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:UCA_Extended_Validation_Root.pem\n",
            "Adding debian:GlobalSign_Root_CA.pem\n",
            "Adding debian:e-Szigno_Root_CA_2017.pem\n",
            "Adding debian:GlobalSign_Root_E46.pem\n",
            "Adding debian:GLOBALTRUST_2020.pem\n",
            "Adding debian:TWCA_Global_Root_CA.pem\n",
            "Adding debian:Buypass_Class_3_Root_CA.pem\n",
            "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
            "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
            "Adding debian:NAVER_Global_Root_Certification_Authority.pem\n",
            "Adding debian:QuoVadis_Root_CA_2.pem\n",
            "Adding debian:COMODO_Certification_Authority.pem\n",
            "Adding debian:OISTE_WISeKey_Global_Root_GC_CA.pem\n",
            "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
            "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
            "Adding debian:AffirmTrust_Commercial.pem\n",
            "Adding debian:Security_Communication_RootCA3.pem\n",
            "Adding debian:Hongkong_Post_Root_CA_3.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:certSIGN_Root_CA_G2.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
            "Adding debian:UCA_Global_G2_Root.pem\n",
            "Adding debian:CFCA_EV_ROOT.pem\n",
            "Adding debian:CA_Disig_Root_R2.pem\n",
            "Adding debian:D-TRUST_EV_Root_CA_1_2020.pem\n",
            "Adding debian:Microsoft_RSA_Root_Certificate_Authority_2017.pem\n",
            "Adding debian:SecureTrust_CA.pem\n",
            "Adding debian:SecureSign_RootCA11.pem\n",
            "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
            "Adding debian:ePKI_Root_Certification_Authority.pem\n",
            "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
            "Adding debian:Telia_Root_CA_v2.pem\n",
            "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
            "Adding debian:vTrus_ECC_Root_CA.pem\n",
            "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:DigiCert_Global_Root_G2.pem\n",
            "Adding debian:Security_Communication_ECC_RootCA1.pem\n",
            "Adding debian:ISRG_Root_X1.pem\n",
            "Adding debian:DigiCert_Global_Root_G3.pem\n",
            "Adding debian:HARICA_TLS_ECC_Root_CA_2021.pem\n",
            "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
            "Adding debian:XRamp_Global_CA_Root.pem\n",
            "Adding debian:DigiCert_TLS_RSA4096_Root_G5.pem\n",
            "Adding debian:Trustwave_Global_ECC_P384_Certification_Authority.pem\n",
            "Adding debian:D-TRUST_BR_Root_CA_1_2020.pem\n",
            "Adding debian:QuoVadis_Root_CA_3.pem\n",
            "Adding debian:Amazon_Root_CA_4.pem\n",
            "Adding debian:Trustwave_Global_Certification_Authority.pem\n",
            "Adding debian:GTS_Root_R3.pem\n",
            "Adding debian:Certigna.pem\n",
            "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
            "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
            "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
            "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
            "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
            "Adding debian:Entrust_Root_Certification_Authority.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
            "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
            "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
            "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G3.pem\n",
            "Adding debian:SSL.com_TLS_RSA_Root_CA_2022.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-01.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_ECC_TLS_2021.pem\n",
            "Adding debian:BJCA_Global_Root_CA1.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_E46.pem\n",
            "Adding debian:BJCA_Global_Root_CA2.pem\n",
            "Adding debian:TrustAsia_Global_Root_CA_G4.pem\n",
            "Adding debian:Sectigo_Public_Server_Authentication_Root_R46.pem\n",
            "Adding debian:SSL.com_TLS_ECC_Root_CA_2022.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-02.pem\n",
            "Adding debian:CommScope_Public_Trust_ECC_Root-02.pem\n",
            "Adding debian:Atos_TrustedRoot_Root_CA_RSA_TLS_2021.pem\n",
            "Adding debian:CommScope_Public_Trust_RSA_Root-01.pem\n",
            "done.\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for ca-certificates (20240203~22.04.1) ...\n",
            "Updating certificates in /etc/ssl/certs...\n",
            "0 added, 0 removed; done.\n",
            "Running hooks in /etc/ca-certificates/update.d...\n",
            "\n",
            "done.\n",
            "done.\n",
            "Install complete.\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 - Install Java, PySpark and required Python packages\n",
        "# Run this cell first. Restart runtime if prompted.\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq\n",
        "!pip install -q pyspark pandas scikit-learn joblib matplotlib seaborn nbformat\n",
        "print(\"Install complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ylkFvYviMBn3",
        "outputId": "5984741d-e8bb-46dc-d868-a8319b4f71d8"
      },
      "id": "ylkFvYviMBn3",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark, os\n",
        "findspark.init()\n",
        "\n",
        "# Check where pyspark is actually installed\n",
        "import pyspark\n",
        "print(\"‚úÖ PySpark Installed At:\", pyspark.__file__)\n",
        "\n",
        "# Extract the folder path automatically\n",
        "spark_path = os.path.dirname(pyspark.__file__)\n",
        "print(\"üìÅ Spark Path Detected:\", spark_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZksirGuLtZJ",
        "outputId": "d85a015f-c825-426a-98d5-c5d71ddd9981"
      },
      "id": "gZksirGuLtZJ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PySpark Installed At: /usr/local/lib/python3.12/dist-packages/pyspark/__init__.py\n",
            "üìÅ Spark Path Detected: /usr/local/lib/python3.12/dist-packages/pyspark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.12/dist-packages/pyspark\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/lib/python3.12/dist-packages/pyspark\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_path, \"bin\")\n",
        "\n",
        "print(\"‚úÖEnvironment variables set successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EsORRV3MRgI",
        "outputId": "695f2371-985f-45c5-c258-794936be1246"
      },
      "id": "_EsORRV3MRgI",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖEnvironment variables set successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder     .appName(\"Smart_Agri_Colab\")     .config(\"spark.ui.enabled\", \"false\")     .getOrCreate()\n",
        "print('Spark version:', spark.version)\n",
        "print('SparkSession created:', spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnemSD8nNpaE",
        "outputId": "c7813b87-5e83-425f-ccf8-0f43c0e03af0"
      },
      "id": "SnemSD8nNpaE",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.5.1\n",
            "SparkSession created: <pyspark.sql.session.SparkSession object at 0x7e9e3e257bf0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "455312f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "455312f9",
        "outputId": "1e021dda-3ff6-4658-86ad-4b21455aff09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bfbecd3d-5c5c-472c-8c0e-8bc4b2fa7618\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bfbecd3d-5c5c-472c-8c0e-8bc4b2fa7618\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Crop_recommendation.csv to Crop_recommendation.csv\n",
            "Uploaded file: Crop_recommendation.csv\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 - Upload your CSV file (Crop_recommendation.csv)\n",
        "# Use the file upload dialog to upload Crop_recommendation.csv and any existing model .pkl if you have one.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # select Crop_recommendation.csv (and optionally crop_recommendation_pipeline.pkl)\n",
        "for fn in uploaded.keys():\n",
        "    print('Uploaded file:', fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ab9cd1b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab9cd1b8",
        "outputId": "cf9d0bc1-6217-43e6-ce18-497b040ed046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema:\n",
            "root\n",
            " |-- N: integer (nullable = true)\n",
            " |-- P: integer (nullable = true)\n",
            " |-- K: integer (nullable = true)\n",
            " |-- temperature: double (nullable = true)\n",
            " |-- humidity: double (nullable = true)\n",
            " |-- ph: double (nullable = true)\n",
            " |-- rainfall: double (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            "\n",
            "Sample rows:\n",
            "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
            "|  N|  P|  K|temperature|   humidity|               ph|   rainfall|label|\n",
            "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
            "| 90| 42| 43|20.87974371|82.00274423|6.502985292000001|202.9355362| rice|\n",
            "| 85| 58| 41|21.77046169|80.31964408|      7.038096361|226.6555374| rice|\n",
            "| 60| 55| 44|23.00445915| 82.3207629|      7.840207144|263.9642476| rice|\n",
            "| 74| 35| 40|26.49109635|80.15836264|      6.980400905|242.8640342| rice|\n",
            "| 78| 42| 42|20.13017482|81.60487287|      7.628472891|262.7173405| rice|\n",
            "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "After cleaning - count: 2200\n",
            "+----+----+----+-----------+-----------+-----------------+-----------+-----+\n",
            "|   N|   P|   K|temperature|   humidity|               ph|   rainfall|label|\n",
            "+----+----+----+-----------+-----------+-----------------+-----------+-----+\n",
            "|90.0|42.0|43.0|20.87974371|82.00274423|6.502985292000001|202.9355362| rice|\n",
            "|85.0|58.0|41.0|21.77046169|80.31964408|      7.038096361|226.6555374| rice|\n",
            "|60.0|55.0|44.0|23.00445915| 82.3207629|      7.840207144|263.9642476| rice|\n",
            "|74.0|35.0|40.0|26.49109635|80.15836264|      6.980400905|242.8640342| rice|\n",
            "|78.0|42.0|42.0|20.13017482|81.60487287|      7.628472891|262.7173405| rice|\n",
            "+----+----+----+-----------+-----------+-----------------+-----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 4 - ETL: Load CSV with Spark, basic cleaning, and inspection\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "CSV_PATH = \"Crop_recommendation.csv\"  # change if your filename differs\n",
        "\n",
        "# Read CSV\n",
        "df = spark.read.csv(CSV_PATH, header=True, inferSchema=True)\n",
        "print('Schema:')\n",
        "df.printSchema()\n",
        "print('Sample rows:')\n",
        "df.show(5)\n",
        "\n",
        "# Basic cleaning: drop nulls in essential columns and cast to double\n",
        "essential = [\"N\",\"P\",\"K\",\"temperature\",\"humidity\",\"ph\",\"rainfall\"]\n",
        "df_clean = df.dropna(subset=essential)\n",
        "for c in essential:\n",
        "    df_clean = df_clean.withColumn(c, col(c).cast(\"double\"))\n",
        "\n",
        "# Filter unrealistic values\n",
        "df_clean = df_clean.filter(\n",
        "    (col(\"ph\") > 0) & (col(\"ph\") < 14) &\n",
        "    (col(\"temperature\") > -30) & (col(\"temperature\") < 60) &\n",
        "    (col(\"rainfall\") >= 0)\n",
        ").cache()\n",
        "\n",
        "print('After cleaning - count:', df_clean.count())\n",
        "df_clean.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "afbc32b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afbc32b2",
        "outputId": "ff809739-966e-4727-fa1f-303cf53a5cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected label column: label\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 - Convert to pandas and prepare features/target\n",
        "pandas_df = df_clean.toPandas()\n",
        "features = [\"N\",\"P\",\"K\",\"temperature\",\"humidity\",\"ph\",\"rainfall\"]\n",
        "\n",
        "# Detect label column common names\n",
        "label_col = None\n",
        "for candidate in [\"label\",\"crop\",\"Crop\",\"CropName\",\"crop_name\"]:\n",
        "    if candidate in pandas_df.columns:\n",
        "        label_col = candidate\n",
        "        break\n",
        "\n",
        "print(\"Detected label column:\", label_col)\n",
        "if label_col is None:\n",
        "    raise ValueError(\"No label column found. Ensure your CSV contains target crop name column (label/crop).\")\n",
        "\n",
        "X = pandas_df[features].copy()\n",
        "y = pandas_df[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "43b3d642",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43b3d642",
        "outputId": "d51024c6-352d-463d-ef47-8c997d428727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9954545454545455\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       apple       1.00      1.00      1.00        20\n",
            "      banana       1.00      1.00      1.00        20\n",
            "   blackgram       1.00      0.95      0.97        20\n",
            "    chickpea       1.00      1.00      1.00        20\n",
            "     coconut       1.00      1.00      1.00        20\n",
            "      coffee       1.00      1.00      1.00        20\n",
            "      cotton       1.00      1.00      1.00        20\n",
            "      grapes       1.00      1.00      1.00        20\n",
            "        jute       0.95      1.00      0.98        20\n",
            " kidneybeans       1.00      1.00      1.00        20\n",
            "      lentil       1.00      1.00      1.00        20\n",
            "       maize       0.95      1.00      0.98        20\n",
            "       mango       1.00      1.00      1.00        20\n",
            "   mothbeans       1.00      1.00      1.00        20\n",
            "    mungbean       1.00      1.00      1.00        20\n",
            "   muskmelon       1.00      1.00      1.00        20\n",
            "      orange       1.00      1.00      1.00        20\n",
            "      papaya       1.00      1.00      1.00        20\n",
            "  pigeonpeas       1.00      1.00      1.00        20\n",
            " pomegranate       1.00      1.00      1.00        20\n",
            "        rice       1.00      0.95      0.97        20\n",
            "  watermelon       1.00      1.00      1.00        20\n",
            "\n",
            "    accuracy                           1.00       440\n",
            "   macro avg       1.00      1.00      1.00       440\n",
            "weighted avg       1.00      1.00      1.00       440\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 6 - Train ML pipeline and evaluate\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=200, random_state=42))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "92758115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "92758115",
        "outputId": "fef2315e-53bf-4fe8-f30a-dae354ec496c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pipeline as crop_recommendation_pipeline_colab.pkl\n",
            "Saved predictions as agri_predictions_colab.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b1179f62-5e6d-483e-bfef-44cee6c56955\", \"crop_recommendation_pipeline_colab.pkl\", 7281730)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f068fae-e3a4-4003-88c1-c8fa1d0b9bfb\", \"agri_predictions_colab.csv\", 178944)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Cell 8 - Save trained pipeline and predictions\n",
        "joblib.dump(pipeline, \"crop_recommendation_pipeline_colab.pkl\")\n",
        "print(\"Saved pipeline as crop_recommendation_pipeline_colab.pkl\")\n",
        "\n",
        "pandas_df[\"Predicted_Crop\"] = pipeline.predict(X)\n",
        "pandas_df.to_csv(\"agri_predictions_colab.csv\", index=False)\n",
        "print(\"Saved predictions as agri_predictions_colab.csv\")\n",
        "\n",
        "# Provide download links\n",
        "from google.colab import files\n",
        "files.download(\"crop_recommendation_pipeline_colab.pkl\")\n",
        "files.download(\"agri_predictions_colab.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7db335bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7db335bd",
        "outputId": "db4d3504-799b-4708-c2fb-8931451658f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote DAG to agri_pipeline_dag.py\n",
            "Wrote example ETL script agri_etl_pipeline.py\n",
            "\n",
            "\n",
            "--- Airflow instructions (local machine) ---\n",
            "\n",
            "1. Install Airflow on your local machine or server (follow official docs).\n",
            "2. Copy agri_pipeline_dag.py to your Airflow DAGs folder.\n",
            "3. Update the bash_command path in the DAG to the full path of agri_etl_pipeline.py\n",
            "4. Ensure spark-submit is available on the machine that runs Airflow (set PATH).\n",
            "5. Start Airflow scheduler and webserver, then trigger the DAG.\n"
          ]
        }
      ],
      "source": [
        "# Cell 9 - Generate Airflow DAG file and instructions\n",
        "# This cell writes a DAG python file you can copy to your Airflow DAGs folder on a local machine or VM.\n",
        "DAG_CONTENT = r\"\"\"\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'ankush',\n",
        "    'depends_on_past': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    dag_id='smart_agri_pipeline',\n",
        "    default_args=default_args,\n",
        "    start_date=datetime(2025, 11, 1),\n",
        "    schedule_interval='@weekly',\n",
        "    catchup=False\n",
        ") as dag:\n",
        "\n",
        "    run_etl = BashOperator(\n",
        "        task_id='run_agri_etl',\n",
        "        bash_command='spark-submit /full/path/to/agri_etl_pipeline.py'\n",
        "    )\n",
        "\n",
        "    run_etl\n",
        "\"\"\"\n",
        "\n",
        "DAG_PATH = 'agri_pipeline_dag.py'\n",
        "with open(DAG_PATH, 'w') as f:\n",
        "    f.write(DAG_CONTENT)\n",
        "print('Wrote DAG to', DAG_PATH)\n",
        "\n",
        "# Also write a sample agri_etl_pipeline.py you can run with spark-submit\n",
        "ETL_SCRIPT = r\"\"\"\n",
        "# agri_etl_pipeline.py - simplified script for spark-submit\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import joblib\n",
        "\n",
        "spark = SparkSession.builder.appName('Smart_Agri_Pipeline').getOrCreate()\n",
        "\n",
        "# Adjust path to your CSV and model\n",
        "RAW_CSV = '/full/path/to/Crop_recommendation.csv'\n",
        "MODEL_PKL = '/full/path/to/crop_recommendation_pipeline_colab.pkl'\n",
        "OUTPUT_PRED_CSV = '/full/path/to/agri_predictions.csv'\n",
        "\n",
        "# Read and clean\n",
        "df = spark.read.csv(RAW_CSV, header=True, inferSchema=True)\n",
        "essential = ['N','P','K','temperature','humidity','ph','rainfall']\n",
        "df_clean = df.dropna(subset=essential)\n",
        "for c in essential:\n",
        "    df_clean = df_clean.withColumn(c, col(c).cast('double'))\n",
        "\n",
        "pandas_df = df_clean.toPandas()\n",
        "model = joblib.load(MODEL_PKL)\n",
        "X = pandas_df[essential]\n",
        "pandas_df['Predicted_Crop'] = model.predict(X)\n",
        "pandas_df.to_csv(OUTPUT_PRED_CSV, index=False)\n",
        "\n",
        "spark.stop()\n",
        "print('ETL + prediction done')\n",
        "\"\"\"\n",
        "\n",
        "with open('agri_etl_pipeline.py', 'w') as f:\n",
        "    f.write(ETL_SCRIPT)\n",
        "print('Wrote example ETL script agri_etl_pipeline.py')\n",
        "\n",
        "print('\\n\\n--- Airflow instructions (local machine) ---\\n')\n",
        "print('1. Install Airflow on your local machine or server (follow official docs).')\n",
        "print('2. Copy agri_pipeline_dag.py to your Airflow DAGs folder.')\n",
        "print('3. Update the bash_command path in the DAG to the full path of agri_etl_pipeline.py')\n",
        "print('4. Ensure spark-submit is available on the machine that runs Airflow (set PATH).')\n",
        "print('5. Start Airflow scheduler and webserver, then trigger the DAG.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}